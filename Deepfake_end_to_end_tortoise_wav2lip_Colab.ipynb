{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reqK2fYU8RMa"
      },
      "source": [
        "Automatic DeepFake Creation (Tortoise voice cloning + wav2lip)\n",
        "\n",
        "Note: A video is required. If an audio is also provided, the voice will be cloned from the audio. The video should have a face looking at all times to the camera.\n",
        "\n",
        "## **Video (and audio) file should be in Google drive in a folder named 'deepfake'. No other files should exist there**\n",
        "\n",
        "wav2lip code taken from https://github.com/snehitvaddi/Deepfake-using-Wave2Lip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jsyyKrp8osG",
        "outputId": "5e64b4b3-637f-4b50-f39f-3614ae52a801"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title Upload video.mp4 (video to overlay voice) & voice.mp3 (voice to clone) files - Should be mp3 and mp4, having any name\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCKcGNEhlP8N",
        "outputId": "7031d953-a65a-47d3-a9ef-63785f576763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/deepfake\n"
          ]
        }
      ],
      "source": [
        "cd gdrive/MyDrive/deepfake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD5EdXykqXXo"
      },
      "outputs": [],
      "source": [
        "base_path='/content/gdrive/MyDrive/deepfake' #Specify path of video/audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODXQ2Hx3RLWv",
        "outputId": "2f447a32-958e-44e2-c634-ce05a855bd6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/253.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m245.8/253.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.7/253.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title Install TTS, pydub to create folders with audio chunks, and moviepy to modify duration of audio/video\n",
        "!pip install -q pydub==0.25.1 TTS==0.22.0 moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NAT2EgsCx_o"
      },
      "outputs": [],
      "source": [
        "#@title English text that we want to read with the cloned voice - This will be inserted in the video too\n",
        "text_to_read=\"Joining two modalities results in a surprising increase in generalization! \\\n",
        "What would happen if we combined them all?\" #Text to read - Greek text will result in error - will try to spell each letter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wntESyuu_jcm"
      },
      "outputs": [],
      "source": [
        "#@title Rename audio and video files to be used below\n",
        "#Prompt: the user selects and uploads to google colab one audio and one video file. Rename the audio file to 'input_voice.mp3' and the video to 'input_video.mp4'\n",
        "import os\n",
        "\n",
        "# Loop over files in the directory\n",
        "for file in os.listdir(os.getcwd()):\n",
        "\n",
        "      filename = os.path.join(base_path, file)\n",
        "\n",
        "      if filename.endswith('.mp3'):\n",
        "          new_filename = 'input_voice.mp3'\n",
        "          os.rename(filename, new_filename)\n",
        "      if filename.endswith('.mp4'):\n",
        "          new_filename = 'video_full.mp4'\n",
        "          os.rename(filename, new_filename)\n",
        "\n",
        "#If only video is provided:\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "def extract_audio(input_video, output_audio):\n",
        "    video = VideoFileClip(input_video)\n",
        "    audio = video.audio\n",
        "    audio.write_audiofile(output_audio)\n",
        "\n",
        "# Provide the input video file path and desired output audio file path\n",
        "input_video = 'video_full.mp4'\n",
        "output_audio = 'input_voice.mp3'\n",
        "\n",
        "#Decide if voice will be cloned from video or audio\n",
        "mp3_check=0\n",
        "for file in os.listdir(os.getcwd()):\n",
        "      file_path =  os.path.join(base_path, file)\n",
        "      if '.mp3' in file_path:\n",
        "        mp3_check=1\n",
        "\n",
        "if mp3_check==0:\n",
        "  print(\"Voice will be cloned from video\")\n",
        "  extract_audio(input_video, output_audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OekWB8DRAUsh",
        "outputId": "fd737705-5dfa-453f-c95b-4ff4103ead2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "121 clips saved in '/content/gdrive/MyDrive/deepfake/voice'.\n"
          ]
        }
      ],
      "source": [
        "#@title Create folder with 10 secs chunks of audio to be used as input in Tortoise\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def split_audio_to_clips(audio_file, output_dir, clip_length=10000, sample_rate=22050):\n",
        "    # Load the audio file\n",
        "    audio = AudioSegment.from_mp3(audio_file)\n",
        "\n",
        "    # Create the output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Calculate the total number of clips\n",
        "    num_clips = len(audio) // clip_length\n",
        "\n",
        "    # Split the audio into clips and save them as WAV files\n",
        "    for i in range(num_clips):\n",
        "        start_time = i * clip_length\n",
        "        end_time = start_time + clip_length\n",
        "        clip = audio[start_time:end_time]\n",
        "\n",
        "        # Set the sample width to 2 bytes for floating-point format\n",
        "        clip = clip.set_sample_width(2)\n",
        "\n",
        "        # Set the sample rate to 22050 Hz\n",
        "        clip = clip.set_frame_rate(sample_rate)\n",
        "\n",
        "        # Save the clip as a WAV file\n",
        "        clip.export(os.path.join(output_dir, f\"{i+1}.wav\"), format=\"wav\")\n",
        "\n",
        "    print(f\"{num_clips} clips saved in '{output_dir}'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Replace 'input_audio.mp3' with the name of your MP3 file\n",
        "    input_audio_file = base_path+'/input_voice.mp3'\n",
        "\n",
        "    # Replace 'voices' with the desired subdirectory name\n",
        "    subdirectory_name = base_path+'/voice'\n",
        "\n",
        "    split_audio_to_clips(input_audio_file, subdirectory_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYRGVrEe75f9",
        "outputId": "bda1c031-76ce-4153-8ee2-a090f3a71cac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > tts_models/en/multi-dataset/tortoise-v2 is already downloaded.\n",
            " > Using model: tortoise\n"
          ]
        }
      ],
      "source": [
        "#@title Download and run TTS tortoise model\n",
        "from TTS.api import TTS\n",
        "tts = TTS(\"tts_models/en/multi-dataset/tortoise-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "b0c2640cdefa4703b190b27b2347cdc1",
            "b02dd0f9050545ad92dd3ac4c2dc796c",
            "c84af149e2724925a17eb7624857c09c",
            "a89b2b191745417d8e3fedb9f5c8be19",
            "227a5c6208434886956023bfeb80f6eb",
            "42f34ef9c1a2471bbde29ace317a8f99",
            "e9622b7e842a407c8d7e710a6a53cb9d",
            "e77e760945464bbe9dc1a5ae1c614b94",
            "8e7a24367bb64f11b53138209903aeea",
            "17c6cc0b21d14c69b75b20251ed6ef62",
            "5a3d9823faf24c97a57d53b7cbb49e9a"
          ]
        },
        "id": "twO-K4WMzDgK",
        "outputId": "024fbdbf-17b7-46ea-cd02-c09f353eccda"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torch/functional.py:650: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:863.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating autoregressive samples..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [01:21<00:00,  5.12s/it]\n",
            "100%|██████████| 16/16 [00:01<00:00, 11.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transforming autoregressive outputs into audio..\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0c2640cdefa4703b190b27b2347cdc1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:<ipython-input-11-fbe285cbd72d>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torchaudio.save(\"tortoise_v2_script.wav\", torch.tensor(output_dict[\"wav\"]).squeeze(0), 24000)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Tortoise Fastest Inference from script (~2min in Colab) - Can also be downloaded from https://huggingface.co/jbetker/tortoise-tts-v2/tree/main\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "from TTS.tts.configs.tortoise_config import TortoiseConfig\n",
        "from TTS.tts.models.tortoise import Tortoise\n",
        "\n",
        "config = TortoiseConfig()\n",
        "model = Tortoise.init_from_config(config)\n",
        "model.load_checkpoint(config, checkpoint_dir=\"/root/.local/share/tts/tts_models--en--multi-dataset--tortoise-v2\", eval=True) #Deepspeed doesn't work\n",
        "model.cuda()\n",
        "\n",
        "# cloning a speaker\n",
        "output_dict = model.synthesize(text_to_read, config, speaker_id=\"voice\", voice_dirs=base_path)\n",
        "\n",
        "#Save result\n",
        "torchaudio.save(\"tortoise_v2_script.wav\", torch.tensor(output_dict[\"wav\"]).squeeze(0), 24000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfnYAcW0_KKZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(base_path+'/tortoise_v2_script.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4o_-0yd-GRC",
        "outputId": "4ed4acd3-2f88-4f69-b46a-ef2fdb899b09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Building video input_video.mp4.\n",
            "MoviePy - Writing audio in input_videoTEMP_MPY_wvf_snd.mp3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video input_video.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready input_video.mp4\n"
          ]
        }
      ],
      "source": [
        "#@title Confirm that audio same length as video. If not, keep the smallest one and cut the other or cut them both to 20secs. This is needed for wav2lip to work\n",
        "#ChatGPT Prompt: Create python code that compares an audio.wav with a video.mp4 files and if the duration of one is bigger than the other,\n",
        "# it cuts the largest one to be the same duration as the smallest. If any of them is bigger than 20secs then raise an error\n",
        "\n",
        "#Needed to avoid errors with encoding\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip\n",
        "\n",
        "def compare_audio_video_duration(audio_file, video_file):\n",
        "    audio = AudioFileClip(audio_file)\n",
        "    video = VideoFileClip(video_file)\n",
        "\n",
        "    audio_duration = audio.duration\n",
        "    video_duration = video.duration\n",
        "\n",
        "    # Either the video or audio should be <20 secs. If any of these is larger than that, it will be cut to the duration of the other or to 20secs.\n",
        "    # Might work for up to 30secs, but not guaranteed. If only video is provided, it will keep only the first 20 secs of it.\n",
        "\n",
        "    # if audio_duration > 20 and video_duration > 20:\n",
        "    #     video = video.subclip(0, 20)\n",
        "    #     video.write_videofile('input_video.mp4')\n",
        "    #     audio = audio.subclip(0, 20)\n",
        "    #     audio.write_audiofile('input_audio.wav')\n",
        "\n",
        "    if audio_duration != video_duration:\n",
        "        min_duration = min(audio_duration, video_duration)\n",
        "        if min_duration == audio_duration:\n",
        "            video = video.subclip(0, min_duration)\n",
        "            video.write_videofile('input_video.mp4')\n",
        "            os.rename(audio_file,'input_audio.wav')\n",
        "        else:\n",
        "            audio = audio.subclip(0, min_duration)\n",
        "            audio.write_audiofile('input_audio.wav')\n",
        "            os.rename(video_file,'input_video.mp4')\n",
        "\n",
        "    audio.close()\n",
        "    video.close()\n",
        "\n",
        "# Example usage\n",
        "compare_audio_video_duration(\"tortoise_v2_script.wav\", \"video_full.mp4\") #input_voice.mp3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT9AQwdf8sJK"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgo-oaI3JU2u",
        "outputId": "f1f15122-2bc4-4e8c-c495-afbbf4545714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "#@title <h1>Install Wav2Lip</h1>\n",
        "#@markdown * Install dependencies\n",
        "#@markdown * Download models\n",
        "# !rm -rf /content/sample_data\n",
        "# !mkdir /content/sample_data\n",
        "\n",
        "!git clone https://github.com/zabique/Wav2Lip\n",
        "\n",
        "#download the pretrained model\n",
        "!wget 'https://iiitaphyd-my.sharepoint.com/personal/radrabha_m_research_iiit_ac_in/_layouts/15/download.aspx?share=EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA' -O '/content/Wav2Lip/checkpoints/wav2lip_gan.pth'\n",
        "!wget 'https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/Eb3LEzbfuKlJiR600lQWRxgBIY27JZg80f7V9jtMfbNDaQ?e=TBFBVW' -O /content/Wav2Lip/checkpoints/wav2lip.pth\n",
        "!pip install https://raw.githubusercontent.com/AwaleSajil/ghc/master/ghc-1.0-py3-none-any.whl\n",
        "\n",
        "# !pip uninstall tensorflow tensorflow-gpu\n",
        "!cd Wav2Lip && pip install -r requirements.txt\n",
        "\n",
        "#download pretrained model for face detection\n",
        "!wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O \"/content/Wav2Lip/face_detection/detection/sfd/s3fd.pth\"\n",
        "\n",
        "!pip install -q youtube-dl\n",
        "!pip install ffmpeg-python\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"\\nDone\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzokJMO19IyY"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgjaWJFs8B38"
      },
      "source": [
        "- Below implementation needs both audio and video to be of same length. Only specific extensions work (mp4 and wav)\n",
        "- Target face in the input_video.mp4, must be \"detectable\" in ALL videoframes (So no black or blurry frames etc)\n",
        "- wav2lip does not like very long and high res clips (1080p/30seconds max)\n",
        "- 'Wav2Lip' model gives highly accurate lip-sync compared to 'Wav2Lip + GAN' but with inferior visual quality compared to the latter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBvt_c2jdebP"
      },
      "source": [
        "Below is needed to fix an error in loading - Not added in the beginning due to conflict in dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "JLVZhHvL7GMo",
        "outputId": "641b98dd-bf2a-4a7a-d7cc-effa69ba002a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting librosa==0.9.1\n",
            "  Using cached librosa-0.9.1-py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: audioread>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (3.0.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (1.22.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (1.4.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (0.4.2)\n",
            "Requirement already satisfied: numba>=0.45.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (1.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.1) (23.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.45.1->librosa==0.9.1) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa==0.9.1) (4.2.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa==0.9.1) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->librosa==0.9.1) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.10.2->librosa==0.9.1) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.1) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1) (2024.2.2)\n",
            "Installing collected packages: librosa\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.0\n",
            "    Uninstalling librosa-0.10.0:\n",
            "      Successfully uninstalled librosa-0.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tts 0.22.0 requires librosa>=0.10.0, but you have librosa 0.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed librosa-0.9.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "librosa"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install librosa==0.9.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR5utmDMcSZY",
        "outputId": "7a2fe45a-bb9d-4a46-8137-55a65486a77c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda for inference.\n",
            "Reading video frames...\n",
            "Number of frames available for inference: 207\n",
            "/content/gdrive/MyDrive/deepfake/Wav2Lip/audio.py:100: FutureWarning: Pass sr=16000, n_fft=800 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  return librosa.filters.mel(hp.sample_rate, hp.n_fft, n_mels=hp.num_mels,\n",
            "(80, 550)\n",
            "Length of mel chunks: 202\n",
            "  0% 0/2 [00:00<?, ?it/s]Downloading: \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" to /root/.cache/torch/hub/checkpoints/s3fd-619a316812.pth\n",
            "\n",
            "  0% 0.00/85.7M [00:00<?, ?B/s]\u001b[A\n",
            "  0% 32.0k/85.7M [00:00<06:36, 226kB/s]\u001b[A\n",
            "  0% 80.0k/85.7M [00:00<05:07, 292kB/s]\u001b[A\n",
            "  0% 160k/85.7M [00:00<03:35, 416kB/s] \u001b[A\n",
            "  0% 336k/85.7M [00:00<02:00, 741kB/s]\u001b[A\n",
            "  1% 688k/85.7M [00:00<01:05, 1.37MB/s]\u001b[A\n",
            "  1% 1.25M/85.7M [00:00<00:38, 2.32MB/s]\u001b[A\n",
            "  3% 2.53M/85.7M [00:01<00:19, 4.59MB/s]\u001b[A\n",
            "  6% 4.77M/85.7M [00:01<00:10, 8.26MB/s]\u001b[A\n",
            "  8% 6.84M/85.7M [00:01<00:07, 10.4MB/s]\u001b[A\n",
            " 11% 9.12M/85.7M [00:01<00:06, 12.3MB/s]\u001b[A\n",
            " 14% 11.6M/85.7M [00:01<00:05, 14.0MB/s]\u001b[A\n",
            " 17% 14.2M/85.7M [00:01<00:04, 15.4MB/s]\u001b[A\n",
            " 20% 16.7M/85.7M [00:01<00:04, 16.3MB/s]\u001b[A\n",
            " 23% 19.3M/85.7M [00:02<00:04, 16.9MB/s]\u001b[A\n",
            " 26% 21.9M/85.7M [00:02<00:03, 17.5MB/s]\u001b[A\n",
            " 29% 24.4M/85.7M [00:02<00:03, 17.8MB/s]\u001b[A\n",
            " 32% 27.2M/85.7M [00:02<00:03, 18.5MB/s]\u001b[A\n",
            " 34% 29.0M/85.7M [00:02<00:03, 16.8MB/s]\u001b[A\n",
            " 37% 31.9M/85.7M [00:02<00:03, 18.0MB/s]\u001b[A\n",
            " 40% 34.4M/85.7M [00:02<00:02, 18.0MB/s]\u001b[A\n",
            " 44% 37.3M/85.7M [00:03<00:02, 18.9MB/s]\u001b[A\n",
            " 47% 40.1M/85.7M [00:03<00:02, 19.3MB/s]\u001b[A\n",
            " 50% 42.7M/85.7M [00:03<00:02, 19.2MB/s]\u001b[A\n",
            " 53% 45.4M/85.7M [00:03<00:02, 19.2MB/s]\u001b[A\n",
            " 56% 48.4M/85.7M [00:03<00:01, 19.9MB/s]\u001b[A\n",
            " 60% 51.0M/85.7M [00:03<00:01, 19.7MB/s]\u001b[A\n",
            " 63% 53.7M/85.7M [00:03<00:01, 19.5MB/s]\u001b[A\n",
            " 66% 56.4M/85.7M [00:04<00:01, 19.5MB/s]\u001b[A\n",
            " 69% 59.2M/85.7M [00:04<00:01, 19.8MB/s]\u001b[A\n",
            " 72% 61.8M/85.7M [00:04<00:01, 19.5MB/s]\u001b[A\n",
            " 75% 64.6M/85.7M [00:04<00:01, 19.8MB/s]\u001b[A\n",
            " 79% 67.3M/85.7M [00:04<00:00, 19.7MB/s]\u001b[A\n",
            " 82% 70.1M/85.7M [00:04<00:00, 19.8MB/s]\u001b[A\n",
            " 85% 72.7M/85.7M [00:04<00:00, 19.5MB/s]\u001b[A\n",
            " 88% 75.2M/85.7M [00:05<00:00, 19.1MB/s]\u001b[A\n",
            " 91% 78.1M/85.7M [00:05<00:00, 19.6MB/s]\u001b[A\n",
            " 94% 80.7M/85.7M [00:05<00:00, 19.4MB/s]\u001b[A\n",
            "100% 85.7M/85.7M [00:05<00:00, 16.2MB/s]\n",
            "\n",
            "  0% 0/13 [00:00<?, ?it/s]\u001b[A"
          ]
        }
      ],
      "source": [
        "#@title Create Wav2Lip video (using wav2lip_gan.pth) GAN\n",
        "!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face '/content/gdrive/MyDrive/deepfake/input_video.mp4' --audio '/content/gdrive/MyDrive/deepfake/input_audio.wav' --resize_factor 2\n",
        "\n",
        "#Use --resize_factor 2 otherwise OOM error. Use resize_factor to reduce the video resolution, as there is a chance you might get better results for lower resolution videos.\n",
        "# This might be related with the model which might have been trained on low resolution faces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxbzXZvLliiA"
      },
      "outputs": [],
      "source": [
        "#@title Play result video -  50% scaling\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('/content/Wav2Lip/results/result_voice.mp4','rb').read()\n",
        "\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"50%\" height=\"50%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kt-krsEbz5j"
      },
      "outputs": [],
      "source": [
        "#@title Download Result.mp4 to your computer\n",
        "# from google.colab import files\n",
        "files.download('/content/Wav2Lip/results/result_voice.mp4') #Only after the last cell is executed this will start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fT8njpBCJ7gD"
      },
      "outputs": [],
      "source": [
        "# #@title Delete old uploaded samples & result files, so you can start over again.\n",
        "# # %rm /content/sample_data/*\n",
        "# %rm /content/Wav2Lip/results/*\n",
        "# from IPython.display import clear_output\n",
        "# clear_output()\n",
        "# print(\"\\nDone! now press X\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7zgfrQqbKom"
      },
      "source": [
        "# **Variations to try**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmcEzPH3TDBg"
      },
      "outputs": [],
      "source": [
        "# #@title Create Wav2Lip video using wav2lip.pth\n",
        "# !cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip.pth --face \"/content/sample_data/input_video.mp4\" --audio \"/content/sample_data/input_audio.wav\"  --resize_factor 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45XW4SZAzIz5"
      },
      "outputs": [],
      "source": [
        "#@title Use more padding to include the chin region (you can manually edit pads dimensions viewing and changing the code)\n",
        "# !cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"/content/sample_data/input_video.mp4\" --audio \"/content/sample_data/input_audio.wav\" --pads 0 20 0 0 --resize_factor 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1Z0zRdZR5BZ"
      },
      "outputs": [],
      "source": [
        "# #@title Play result video -  50% scaling\n",
        "# from IPython.display import HTML\n",
        "# from base64 import b64encode\n",
        "# mp4 = open('/content/Wav2Lip/results/result_voice.mp4','rb').read()\n",
        "# data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "# HTML(f\"\"\"\n",
        "# <video width=\"50%\" height=\"50%\" controls>\n",
        "#       <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "# </video>\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfXFOpAmR_dh"
      },
      "outputs": [],
      "source": [
        "#@title Download Result.mp4 to your computer\n",
        "# from google.colab import files\n",
        "# files.download('/content/Wav2Lip/results/result_voice.mp4')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "17c6cc0b21d14c69b75b20251ed6ef62": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "227a5c6208434886956023bfeb80f6eb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42f34ef9c1a2471bbde29ace317a8f99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a3d9823faf24c97a57d53b7cbb49e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e7a24367bb64f11b53138209903aeea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a89b2b191745417d8e3fedb9f5c8be19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17c6cc0b21d14c69b75b20251ed6ef62",
            "placeholder": "​",
            "style": "IPY_MODEL_5a3d9823faf24c97a57d53b7cbb49e9a",
            "value": " 100/100 [00:21&lt;00:00,  4.72it/s]"
          }
        },
        "b02dd0f9050545ad92dd3ac4c2dc796c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42f34ef9c1a2471bbde29ace317a8f99",
            "placeholder": "​",
            "style": "IPY_MODEL_e9622b7e842a407c8d7e710a6a53cb9d",
            "value": "100%"
          }
        },
        "b0c2640cdefa4703b190b27b2347cdc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b02dd0f9050545ad92dd3ac4c2dc796c",
              "IPY_MODEL_c84af149e2724925a17eb7624857c09c",
              "IPY_MODEL_a89b2b191745417d8e3fedb9f5c8be19"
            ],
            "layout": "IPY_MODEL_227a5c6208434886956023bfeb80f6eb"
          }
        },
        "c84af149e2724925a17eb7624857c09c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e77e760945464bbe9dc1a5ae1c614b94",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e7a24367bb64f11b53138209903aeea",
            "value": 100
          }
        },
        "e77e760945464bbe9dc1a5ae1c614b94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9622b7e842a407c8d7e710a6a53cb9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
